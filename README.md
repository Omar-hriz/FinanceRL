#  BaseMarketEnv 
Cet environnement est basÃ© sur lâ€™article *"Deep Reinforcement Learning for Trading"* (Zhang et al., 2020), et sert de socle pour entraÃ®ner un agent Ã  trader un actif financier Ã  lâ€™aide du renforcement.

---

##  FonctionnalitÃ©s principales

- Compatible `gymnasium`
- GÃ¨re un portefeuille composÃ© de :
  - ğŸ’µ cash
  - ğŸ“ˆ position (quantitÃ© d'actif dÃ©tenue)
- Ã‰value dynamiquement la valeur du portefeuille et gÃ©nÃ¨re une rÃ©compense basÃ©e sur les profits nets

---

##  ParamÃ¨tres d'initialisation

```python
BaseMarketEnv(df, initial_cash=1000.0, trading_cost=0.001)
```

- `df` : DataFrame avec au moins une colonne `'Close'`
- `initial_cash` : Cash initial (par dÃ©faut 1000)
- `trading_cost` : Frais de transaction appliquÃ© Ã  chaque action (hors hold)

---

##  Structure MDP

### ğŸ” Observation (`observation_space`)
Vecteur contenant :
- DonnÃ©es du marchÃ© Ã  lâ€™instant `t` (`df.iloc[current_step]`)
- QuantitÃ© dâ€™actif dÃ©tenue
- Ratio `cash / portefeuille`

### ğŸ® Action (`action_space`)
- `0` : Short â†’ vendre tout
- `1` : Hold â†’ ne rien faire
- `2` : Long â†’ acheter tout avec le cash

### ğŸ’° RÃ©compense (`_compute_reward`)
```python
reward = (portfolio_value - initial_cash) / initial_cash
reward -= trading_cost * abs(action - 1)
```
Pas de frais si lâ€™agent choisit **Hold** (`action = 1`).

---

##  Fonctionnement

1. `reset()` : rÃ©initialise l'environnement
2. `step(action)` :
   - applique l'action
   - met Ã  jour l'Ã©tat du portefeuille
   - avance Ã  la ligne suivante des donnÃ©es
   - retourne : `observation, reward, done, info`

---

##  Info retournÃ©e

Chaque step retourne un dictionnaire :

```python
{
  'portfolio': valeur totale (cash + actifs),
  'cash': liquiditÃ© disponible,
  'position': nombre dâ€™unitÃ©s dâ€™actif dÃ©tenues
}
```

---

##  Conditions requises

- `df` doit contenir une colonne `'Close'`
- Les autres colonnes (Volume, RSI, etc.) peuvent enrichir lâ€™observation

---

##  Exemple dâ€™utilisation

```python
import pandas as pd
from envs.base_market_env import BaseMarketEnv

df = pd.read_csv("data/AAPL.csv")
env = BaseMarketEnv(df)

obs = env.reset()
done = False
while not done:
    action = env.action_space.sample()
    obs, reward, done, info = env.step(action)
    print(reward, info)
```
